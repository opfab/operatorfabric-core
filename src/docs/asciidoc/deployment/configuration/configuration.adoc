// Copyright (c) 2018-2022 RTE (http://www.rte-france.com)
// See AUTHORS.txt
// This document is subject to the terms of the Creative Commons Attribution 4.0 International license.
// If a copy of the license was not distributed with this
// file, You can obtain one at https://creativecommons.org/licenses/by/4.0/.
// SPDX-License-Identifier: CC-BY-4.0




:springboot_doc: https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/
:mongo_doc: https://docs.mongodb.com/manual/reference/
:spring_kafka_doc: https://docs.spring.io/spring-kafka/reference/html/

= Configuration

OperatorFabric has multiple services to configure. 

See the
ifdef::single-page-doc[<<architecture, architecture documentation>>]
ifndef::single-page-doc[<</documentation/current/architecture/index.adoc#architecture, architecture documentation>>]
for more information on the different services.

All services are SpringBoot applications and use jetty as an embedded servlet container. As such, they share some common
configuration which is described in the following documentation:

 * link:{springboot_doc}/reference/htmlsingle/[Springboot documentation]
 * link:{springboot_doc}/reference/htmlsingle/#boot-features-external-config[Springboot external configuration]
 * link:{springboot_doc}/reference/htmlsingle/#common-application-properties[Common application properties from Springboot documentation]

Configuration is centralized in the *config* directory, the *dev* subdirectory is specific to development environments
while the *docker* subdirectory is a specific configuration meant for use in a full docker environment.

== Business service configuration

=== Specify an external configuration

When starting docker in the full docker environment an external environment file could be provided like:

----
cd ./config/docker
./docker-compose.sh ~/config/local.env
----
In the provided environment file the Spring active profiles can be set.

----
SPRING_PROFILES_ACTIVE=docker,local
----
This way the configuration file 'cards-publication-local.yml' can be provided in the same configuration directory and can be read by SpringBoot


=== Shared service configuration

The configuration shared by all services is in a yaml file, you can find an example with the file
/config/docker/common-docker.yml.
In this file you will find, among others, the parameters below :

|===
|name|default|mandatory?|Description


|operatorfabric.servicesUrls.users||yes|Indicates where the Users service can we reached (to get information about the current user).
|operatorfabric.userActionLogActivated|yes|no|Indicates if the user action log feature is enabled or not.

|===

=== Service specific configurations

Each service has a specific yaml configuration file. It should a least contain the name of the service:

[source, yaml]
----
spring:
  application:
    name: businessconfig
----

Examples of configuration of each service can be found either under config/docker or config/dev depending on
the type of deployment you're looking for.

==== Businessconfig service

The businessconfig service has this specific property : 

|===
|name|default|mandatory?|Description

|operatorfabric.businessconfig.storage.path|null|no|File path to data storage folder

|===


==== Users service

The user service has these specific properties :

|===
|name|default|mandatory?|Description

|operatorfabric.users.default.users|null|no| Array of user objects to create upon startup if they don't exist
|operatorfabric.users.default.user-settings|null|no| Array of user settings objects to create upon startup if they don't exist
|operatorfabric.users.default.groups|null|no| Array of group objects to create upon startup if they don't exist
|operatorfabric.users.default.entities|null|no| Array of entity objects to create upon startup if they don't exist

|===

[[cards-pub-conf]]
==== Cards-publication service

The cards-publication service has these specific properties :

|===
|name|default|mandatory?|Description

|checkAuthenticationForCardSending|true|no|If false, OperatorFabric will not require user authentication to send or delete a card via endpoint /cards (it does not concern user cards which always need authentication). Be careful when setting the value to false, nginx conf must be adapted for security reasons (see security warning in link:https://github.com/opfab/operatorfabric-core/blob/master/config/docker/nginx.conf[the reference nginx.conf])
|authorizeToSendCardWithInvalidProcessState|false|no|If true, OperatorFabric will allow to publish a card referring a not existent process or state 
|checkPerimeterForCardSending|true|no|If true, OperatorFabric will check user perimeter for card sending via endpoint /cards (it does not concern user cards which are always controlled).
|spring.kafka.consumer.group-id |null|no| If set, support for receiving cards via Kafka is enabled
|spring.deserializer.value.delegate.class|io.confluent.kafka.serializers.
KafkaAvroDeserializer|yes| Deserializer used to convert the received bytes into objects
|spring.serializer.value.delegate.class |io.confluent.kafka.serializers.
KafkaAvroSerializer|yes|Serializer used to convert cards to bytes
|spring.kafka.producer.bootstrap-servers|http://localhost:9092|no|comma separated list of URL(s) of the broker(s) / bootstrap server(s)
|opfab.kafka.topics.card.topicname |opfab|no|Name of the topic to read the messages from
|opfab.kafka.topics.response-card.topicname |opfab|no|Name of the topic to place the response cards to
|opfab.kafka.schema.registry.url|http://localhost:8081|yes|URL of the schema registry. Can be set to the empty string "" is no registry is used
|delayForDeleteExpiredCardsScheduling|60000|no|The delay in millisecond after the last execution finished and the next execution starts.
|===


==== Cards-consultation service

The cards-consultation service has these specific properties :

|===
|name|default|mandatory?|Description

|checkIfUserIsAlreadyConnected|true|no|If false, OperatorFabric will allow a user to have several sessions opened at the same time. However, it may cause synchronization problems between the sessions using the same login, so it is recommended to let it to true, its default value. 
|===

[[external-devices-conf]]
==== External devices service

The external devices service can be configured with the following properties:

|===
|name|default|mandatory?|Description

|operatorfabric.externaldevices.watchdog.enabled|false|no|If true, watchdog signals will be sent to external devices to show that the OperatorFabric is running and connected.
|operatorfabric.externaldevices.watchdog.cron|`*/5 * * * * *`|no|CRON expression determining when watchdog signals should be sent to external devices.
|operatorfabric.externaldevices.watchdog.signalId|0|no|Id the signal the external devices are expecting as watchdog
|===

include::web-ui_configuration.adoc[leveloffset=+1]



[[opfab_spec_conf]]
include::security_configuration.adoc[leveloffset=+1]

== OperatorFabric Mongo configuration

We only use URI configuration for mongo through the usage of the ```spring.data.mongodb.uris```,
it allows us to share the same configuration behavior for simple or cluster
configuration and with both spring classic and reactive mongo configuration.
See link:{mongo_doc}connection-string/[mongo connection string] for the complete URI syntax.

=== Define time to live for archived cards

By default, archived cards will remain stored in the database forever. It is possible to have them automatically
removed after a specified duration by using the link:https://docs.mongodb.com/manual/core/index-ttl/[TTL index feature of mongoDB] on their publishDate field.

For example, to have cards expire after 10 days (864000s), enter the following commands in the mongo shell:

[source,shell]
----
use operator-fabric
db.archivedCards.createIndex( { "publishDate": 1 }, { expireAfterSeconds: 864000 } )
----

IMPORTANT: You cannot use createIndex() to change the value of expireAfterSeconds of an existing index.
Instead use the link:https://docs.mongodb.com/manual/reference/command/collMod/#dbcmd.collMod[collMod] database command in conjunction with the index collection flag. Otherwise, to
change the value of the option of an existing index, you must drop the index first and recreate.

== OperatorFabric Kafka configuration

Next to publishing cards to OperatorFabric using the REST API, OperatorFabric also supports publishing cards via a Kafka Topic. In the default configuration Kafka is disabled.
To enable Kafka you need to set the consumer group to the consumer group you assign to the OpFab Kafka consumer. This can be any group-id, as long as it isn't used by other consumers
(unless you explicitly want multiple consumers for the same group).

You can set the group_id by uncommenting the `kafka.consumer.group_id` in the `cards-publication.yml`

[source, yaml]
----
  kafka:
    consumer:
      group-id: opfab-command
----
By default, the consumer will consume messages from the `opfab` topic.
See link:{spring_kafka_doc}[Spring for Apache Kafka] for more information on the Spring Kafka implementation.

With the default settings, the Kafka consumer expects a broker running on http//127.0.0.1:9092 and a schema registry on http://127.0.0.1:8081.

Operator Fabric is also able to publish response cards to a Kafka topic. The default topic name  `opfab-response`. You can specify which response cards
are to be returned via Kafka by setting the `external-recipients` in the `cards-publication` yaml file. Instead of setting `http://` URL you should set it to `kafka:`

[source, yaml]
----
external-recipients:
  recipients: 
    - id: "processAction"
      url: "http://localhost:8090/test"
      propagateUserToken: true
    - id: "mykafka"
      url: "kafka:topicname"
      propagateUserToken: false

----

Note that `topicname` is a placeholder for now. All response cards are returned via the same Kafka response topic, as specified in the `opfab.kafka.topics.response-card` field.

Also note enabling Kafka does not disable the REST interface.

Example Kafka configuration plain:
[source, yaml]
----
spring:
  application:
    name: cards-publication
  deserializer:
    value:
      delegate:
        class: org.opfab.cards.publication.kafka.consumer.KafkaAvroWithoutRegistryDeserializer
  serializer:
    value:
      delegate:
        class: org.opfab.cards.publication.kafka.producer.KafkaAvroWithoutRegistrySerializer
  kafka:
    consumer:
      group-id: OPFAB
      properties:
        specific:
          avro:
            reader: true
    producer:
      client-id: operatorfabric-producer
    bootstrap-servers: kafka-server:9092
opfab:
  kafka:
    topics:
      card:
        topicname: m_opfab-card-commands_dev
      response-card:
        topicname: m_opfab-card-response_dev
----

Example Kafka configuration SASL:
[source, yaml]
----
spring:
  application:
    name: cards-publication
  deserializer:
    value:
      delegate:
        class: org.opfab.cards.publication.kafka.consumer.KafkaAvroWithoutRegistryDeserializer
  serializer:
    value:
      delegate:
        class: org.opfab.cards.publication.kafka.producer.KafkaAvroWithoutRegistrySerializer
  kafka:
    consumer:
      group-id: OPFAB
      security:
        protocol: SASL_SSL
      properties:
        specific:
          avro:
            reader: true
        sasl:
          mechanism: SCRAM-SHA-256
          jaas:
            config: org.apache.kafka.common.security.scram.ScramLoginModule required username="kafkaUsername" password="kafkaPassword";
    producer:
      client-id: operatorfabric-producer
      security:
        protocol: SASL_SSL
      properties:
        sasl:
          mechanism: SCRAM-SHA-256
          jaas:
            config: org.apache.kafka.common.security.scram.ScramLoginModule required username="kafkaUsername" password="kafkaPassword";
    bootstrap-servers: kafka-server:9094
    ssl:
      trust-store-type: PKCS12
      trust-store-password: truststorePassword
      trust-store-location: file:///etc/truststore.pkcs
    properties:
      ssl:
        endpoint:
          identification:
            algorithm: ""
opfab:
  kafka:
    topics:
      card:
        topicname: opfab-card-commands
      response-card:
        topicname: opfab-card-response
----

Example Kafka configuration Kerberos:
[source, yaml]
----
spring:
  application:
    name: cards-publication
  deserializer:
    key:
      delegate:
        class: org.apache.kafka.common.serialization.StringDeserializer
    value:
      delegate:
        class: org.opfab.cards.publication.kafka.consumer.KafkaAvroWithoutRegistryDeserializer
  serializer:
    value:
      delegate:
        class: org.opfab.cards.publication.kafka.producer.KafkaAvroWithoutRegistrySerializer
  kafka:
    security:
      protocol: SASL_SSL
    properties:
      sasl.mechanism: GSSAPI
      sasl:
        jaas:
          config: com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab="/etc/kafkaUsername.keytab" storeKey=true useTicketCache=false serviceName="kafka" principal="kafkaUsername@DOMAIN";
    bootstrap-servers: kafka-server:9094
    ssl:
      trust-store-type: pkcs12
      trust-store-password: truststorePassword
      trust-store-location: file:///etc/truststore.pkcs12
    consumer:
      group-id: OPFAB
      key-deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
      properties:
        spring:
          deserializer:
            key:
              delegate:
                class: org.apache.kafka.common.serialization.StringDeserializer
            value:
              delegate:
                class: org.opfab.cards.publication.kafka.consumer.KafkaAvroWithoutRegistryDeserializer
    producer:
      client-id: OPFAB
      value-serializer: org.opfab.cards.publication.kafka.producer.KafkaAvroWithoutRegistrySerializer
opfab:
  kafka:
    topics:
      card:
        topicname: opfab-card-commands
      response-card:
        topicname: opfab-card-response
----
